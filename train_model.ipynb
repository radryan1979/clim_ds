{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import models2 as models\n",
    "import loss as loss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytorch_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------#\n",
    "\n",
    "UPSCALE_FACTOR = 2\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "base_folder = '/media/ryan/DataDrive/clim_model_runs/'\n",
    "out_path = '/media/ryan/DataDrive/clim_model_runs/'\n",
    "run_number = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------#\n",
    "USE_GPU = False\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 1\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "train_directory = '/media/ryan/DataDrive/clim_tif/train'\n",
    "test_directory = '/media/ryan/DataDrive/clim_tif/test'\n",
    "val_directory = '/media/ryan/DataDrive/clim_tif/val'\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "#--------------------------------------------------#\n",
    "\n",
    "sr_train = models.SR_Dataset(train_directory)\n",
    "loader_train = DataLoader(sr_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "sr_val = models.SR_Dataset(val_directory)\n",
    "loader_val = DataLoader(sr_val, batch_size=BATCH_SIZE)\n",
    "\n",
    "sr_test = models.SR_Dataset(test_directory)\n",
    "loader_test = DataLoader(sr_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "#--------------------------------------------------#\n",
    "netG = models.Generator(UPSCALE_FACTOR)\n",
    "print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
    "print(netG)\n",
    "netD = models.Discriminator()\n",
    "print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
    "print(netD)\n",
    "\n",
    "# these explode before the first epoch is even done\n",
    "# optimizerG = optim.Adam(netG.parameters(), lr=1e-5, betas=(0.5, 0.9))\n",
    "# optimizerD = optim.Adam(netD.parameters(), lr=1e-5, betas=(0.5, 0.9))\n",
    "\n",
    "optimizerG = optim.SGD(netG.parameters(), lr=1e-5)\n",
    "optimizerD = optim.SGD(netD.parameters(), lr=1e-5)\n",
    "\n",
    "mseLoss = nn.MSELoss()\n",
    "\n",
    "#---------------------------------------------------#\n",
    "\n",
    "results = {\n",
    "        'd_loss_real':[],\n",
    "        'd_loss_fake':[],\n",
    "        'd_loss_total':[],\n",
    "        'g_loss_content':[],\n",
    "        'g_loss_adv':[],\n",
    "        'g_loss_total':[],\n",
    "        'psnr':[],\n",
    "        'ssim':[]\n",
    "    }\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(epoch)\n",
    "    train_bar = tqdm(loader_train)\n",
    "    # running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "\n",
    "    running_results = {\n",
    "        'batch_sizes':0,\n",
    "        'd_loss_real':0,\n",
    "        'd_loss_fake':0,\n",
    "        'd_loss_total':0,\n",
    "        'g_loss_content':0,\n",
    "        'g_loss_adv':0,\n",
    "        'g_loss_total':0\n",
    "    }\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    \n",
    "    G_content = np.zeros(len(loader_train)*NUM_EPOCHS+1)\n",
    "    G_advers = np.zeros(len(loader_train)*NUM_EPOCHS+1)\n",
    "    D_real_L = np.zeros(len(loader_train)*NUM_EPOCHS+1)\n",
    "    D_fake_L = np.zeros(len(loader_train)*NUM_EPOCHS+1)\n",
    "    \n",
    "    iter_count = 0\n",
    "    for data, target in train_bar:\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "\n",
    "        real_img = target\n",
    "        logits_real = netD(real_img)\n",
    "        fake_img = netG(data)\n",
    "        logits_fake = netD(fake_img)\n",
    "        \n",
    "        # Update for the discriminator\n",
    "        d_loss, D_real_L[iter_count], D_fake_L[iter_count] = loss.discriminator_loss(logits_real, logits_fake)\n",
    "        optimizerD.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        fake_images = netG(data)\n",
    "        logits_fake = netD(fake_images)\n",
    "        gen_logits_fake = netD(fake_images)\n",
    "        weight_param = 1e-1 # Weighting put on adversarial loss\n",
    "        g_error, G_content[iter_count], G_advers[iter_count] = loss.generator_loss(fake_images, real_img, gen_logits_fake, weight_param=weight_param)\n",
    "        optimizerG.zero_grad()\n",
    "        g_error.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # loss for current batch before optimization \n",
    "        running_results['d_loss_real'] += D_real_L[iter_count]\n",
    "        running_results['d_loss_fake'] += D_fake_L[iter_count]\n",
    "        running_results['d_loss_total'] += d_loss\n",
    "        running_results['g_loss_content'] += G_content[iter_count]\n",
    "        running_results['g_loss_adv'] += G_advers[iter_count]\n",
    "        running_results['g_loss_total'] += g_error\n",
    "\n",
    "\n",
    "\n",
    "        train_bar.set_description(desc='[%d/%d] D Loss Real: %.4f D Loss Fake: %.4f D Loss Total: %.4f G Loss Content: %.4f G Loss Adv: %.4f G Loss Total: %.4f' % (\n",
    "            epoch, NUM_EPOCHS,\n",
    "            running_results['d_loss_real'] / running_results['batch_sizes'],\n",
    "            running_results['d_loss_fake'] / running_results['batch_sizes'],\n",
    "            running_results['d_loss_total'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss_content'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss_adv'] / running_results['batch_sizes'],\n",
    "            running_results['g_loss_total'] / running_results['batch_sizes']\n",
    "        ))\n",
    "        \n",
    "        iter_count += 1\n",
    "\n",
    "    # put the model in eval model\n",
    "    # this whole section is needs a rework - model output (mode2) isn't right\n",
    "    netG.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(loader_val)\n",
    "        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "        val_images = []\n",
    "        for val_lr, val_hr in val_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results['batch_sizes'] += batch_size\n",
    "            lr = val_lr\n",
    "            hr = val_hr\n",
    "            if torch.cuda.is_available():\n",
    "                lr = lr.cuda()\n",
    "                hr = hr.cuda()\n",
    "            sr = netG(lr)\n",
    "\n",
    "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "            valing_results['mse'] += batch_mse * batch_size\n",
    "            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "            valing_results['ssims'] += batch_ssim * batch_size\n",
    "            valing_results['psnr'] = 10 * np.log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "            val_bar.set_description(\n",
    "                desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                    valing_results['psnr'], valing_results['ssim']\n",
    "                )\n",
    "            )\n",
    "\n",
    "            val_images.extend(\n",
    "                 [   lr.data.cpu().squeeze(0),\n",
    "                     hr.data.cpu().squeeze(0), \n",
    "                     sr.data.cpu().squeeze(0)]\n",
    "             )\n",
    "            \n",
    "        val_images = torch.stack(val_images)\n",
    "        val_images = torch.chunk(val_images, val_images.size(0) // 20)\n",
    "        val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
    "        # index = 1\n",
    "        # print(f'Length of val_save_bar: {0}',len(val_save_bar))\n",
    "        # for image in val_save_bar:\n",
    "        #     #print(f'Image index: {index}')\n",
    "        #     #print(image[0].shape)\n",
    "        #     #print(image[1].shape)\n",
    "        #     #print(image[3].shape)\n",
    "        #     plt.subplot(131)\n",
    "        #     plt.imshow(image[0][1,0,:,:])\n",
    "        #     plt.title('Low Res Image')\n",
    "        #     plt.subplot(132)\n",
    "        #     plt.imshow(image[1][1,0,:,:])\n",
    "        #     plt.title('High Res Image')\n",
    "        #     plt.subplot(133)\n",
    "        #     plt.imshow(image[2][1,0,:,:])\n",
    "        #     plt.title('SR Image')\n",
    "        #     saveloc = os.path.join(out_path,f'epoch_{epoch}_index_{index}.png')\n",
    "        #     plt.savefig(saveloc)\n",
    "        #     index += 1\n",
    "\n",
    "    # save model parameters\n",
    "    gmodel = os.path.join(out_path, f'epochs/netG_run_{run_number}_epoch_{epoch}.pth')\n",
    "    dmodel = os.path.join(out_path, f'epochs/netD_run_{run_number}_epoch_{epoch}.pth')\n",
    "    \n",
    "    torch.save(netG.state_dict(), gmodel)\n",
    "    torch.save(netD.state_dict(), dmodel)\n",
    "    \n",
    "    \n",
    "    # save loss\\scores\\psnr\\ssim\n",
    "    \n",
    "    results['d_loss_real'].append(running_results['d_loss_real'] / running_results['batch_sizes'])\n",
    "    results['d_loss_fake'].append(running_results['d_loss_fake'] / running_results['batch_sizes'])\n",
    "    results['d_loss_total'].append(running_results['d_loss_total'] / running_results['batch_sizes'])\n",
    "    results['g_loss_content'].append(running_results['g_loss_content'] / running_results['batch_sizes'])\n",
    "    results['g_loss_adv'].append(running_results['g_loss_adv'] / running_results['batch_sizes'])\n",
    "    results['g_loss_total'].append(running_results['g_loss_total'] / running_results['batch_sizes'])\n",
    "    results['psnr'].append(valing_results['psnr'])\n",
    "    results['ssim'].append(valing_results['ssim'])\n",
    "\n",
    "    stat_path = os.path.join(out_path,\"statistics/\")\n",
    "    print(stat_path)\n",
    "    data_frame = pd.DataFrame(\n",
    "            data = {\n",
    "                'd_loss_real':results['d_loss_real'],\n",
    "                'd_loss_fake':results['d_loss_fake'],\n",
    "                #'d_loss_total':results['d_loss_total'],\n",
    "                'g_loss_content':results['g_loss_content'],\n",
    "                'g_loss_adv':results['g_loss_adv'],\n",
    "                #'g_loss_total':results['g_loss_total']\n",
    "                'PSNR': results['psnr'], \n",
    "                'SSIM': results['ssim']\n",
    "                }, \n",
    "            index=range(1, epoch+1)\n",
    "    )\n",
    "    stat_file = f'stats_epoch{epoch}_run{run_number}.csv'\n",
    "    data_frame.to_csv(os.path.join(stat_path, stat_file), index_label='Epoch')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
